{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM0U45ZsuOu3956VTHjj493",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RahmaMissaoui/Facial_Rwcognition/blob/main/Facial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Homework 2: Facial Recognition Using CNN**\n",
        "**Group Members**: Rahma Missaoui & Khettabi Fadila Meissoune   \n",
        "**Date**: January 10 20026"
      ],
      "metadata": {
        "id": "-hJFcJQV8IwR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Objective**\n",
        "This work aims to train a **convolutional neural network** (CNN) that predicts the *boundary* of a face in an image.\n",
        "\n",
        "Instead of classifying images, the model performs a `regression` process and outputs four connected values ​​representing the face's `position`."
      ],
      "metadata": {
        "id": "EK1H_h6D2KB2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Dataset**\n",
        "\n",
        "It makes use of the **WIDER FACE** dataset.\n",
        "To keep things simple, only the `first face` in each picture is taken into account. Multiple-face images are not fully utilized."
      ],
      "metadata": {
        "id": "Q2C3L7N18v1c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Models compared**\n",
        "1. CNN from Scratch - Simple 3-layer convolutional network\n",
        "2. VGG16 - Pre-trained on ImageNet, fine-tuned for face detection\n",
        "3. ResNet50 - Pre-trained on ImageNet, fine-tuned for face detection"
      ],
      "metadata": {
        "id": "sNCAMbbSu-90"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ],
      "metadata": {
        "id": "Mp0o2HzcvVy2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "from pathlib import Path\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import cv2\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.applications import VGG16, ResNet50\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "vHl11IBOvTMn"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download dataset"
      ],
      "metadata": {
        "id": "dalMmpvNEISD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download WIDER FACE dataset\n",
        "path_str = kagglehub.dataset_download(\"iamprateek/wider-face-a-face-detection-dataset\")\n",
        "path = Path(path_str)\n",
        "\n",
        "# Define paths\n",
        "img_base_dir = path / 'WIDER_train' / 'WIDER_train' / 'images'\n",
        "annot_file = path / 'wider_face_annotations' / 'wider_face_split' / 'wider_face_train_bbx_gt.txt'\n",
        "\n",
        "print(\"Images folder exists:\", img_base_dir.exists())\n",
        "print(\"Annotations file exists:\", annot_file.exists())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xwi4LmNl8-X0",
        "outputId": "cd73a971-71e1-47ad-9fa2-f2079e56db17"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'wider-face-a-face-detection-dataset' dataset.\n",
            "Images folder exists: True\n",
            "Annotations file exists: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration\n",
        "IMG_SIZE = 224\n",
        "BATCH_SIZE = 16"
      ],
      "metadata": {
        "id": "Prlq-lC-vwgG"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Parse annotations"
      ],
      "metadata": {
        "id": "spVV8e5crdPT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "    Parse WIDER FACE annotation file.\n",
        "    Format can have empty lines and special cases.\n",
        "    Structure: image_path, num_faces, then bbox lines (x y w h + additional attributes)\n"
      ],
      "metadata": {
        "id": "rRWdVtrTwMAi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_wider_face(annot_file, img_dir, max_samples=None):\n",
        "\n",
        "    samples = []\n",
        "\n",
        "    with open(annot_file, 'r') as f:\n",
        "        lines = [line.strip() for line in f.readlines() if line.strip()]  # Remove empty lines\n",
        "\n",
        "    i = 0\n",
        "    while i < len(lines):\n",
        "        # Read image filename (contains .jpg)\n",
        "        img_name = lines[i]\n",
        "        i += 1\n",
        "\n",
        "        if i >= len(lines):\n",
        "            break\n",
        "\n",
        "        # Try to read number of faces\n",
        "        try:\n",
        "            num_faces = int(lines[i])\n",
        "            i += 1\n",
        "        except ValueError:\n",
        "            # If can't parse as int, might be another image name\n",
        "            continue\n",
        "\n",
        "        # If there are faces, extract the first one\n",
        "        if num_faces > 0 and i < len(lines):\n",
        "            try:\n",
        "                parts = lines[i].split()\n",
        "                if len(parts) >= 4:\n",
        "                    x, y, w, h = map(int, parts[:4])\n",
        "                    # Only add valid bounding boxes (positive width/height)\n",
        "                    if w > 0 and h > 0:\n",
        "                        samples.append((img_name, x, y, w, h))\n",
        "            except (ValueError, IndexError):\n",
        "                pass\n",
        "\n",
        "        # Skip remaining face annotations for this image\n",
        "        i += num_faces\n",
        "\n",
        "        if max_samples and len(samples) >= max_samples:\n",
        "            break\n",
        "\n",
        "    return samples"
      ],
      "metadata": {
        "id": "9fVY8-XCrzpG"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load 5000 samples for training\n",
        "samples = parse_wider_face(annot_file, img_base_dir, max_samples=5000)\n",
        "\n",
        "# Split: 80% training, 20% validation\n",
        "train_samples, val_samples = train_test_split(samples, test_size=0.2, random_state=42)\n",
        "print(f\"Training samples: {len(train_samples)}\")\n",
        "print(f\"Validation samples: {len(val_samples)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "poxXATLCwRmJ",
        "outputId": "d97015f8-3368-4bed-d3de-d6df8a37a1b2"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training samples: 4000\n",
            "Validation samples: 1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PREPROCESSING\n",
        "\n",
        "    Load image and normalize both image and bounding box.\n",
        "    \n",
        "    Process:\n",
        "    1. Load image and resize to 224x224\n",
        "    2. Normalize pixel values to [0, 1]\n",
        "    3. Normalize box coordinates to [0, 1]\n",
        "    \n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "s8bNJrSxTDvY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_preprocess(sample, img_dir):\n",
        "\n",
        "    img_name, x, y, w, h = sample\n",
        "    img_path = img_dir / img_name\n",
        "\n",
        "    # Load image with error checking\n",
        "    image = cv2.imread(str(img_path))\n",
        "\n",
        "    # Check if image loaded successfully\n",
        "    if image is None:\n",
        "        raise FileNotFoundError(f\"Could not load image: {img_path}\")\n",
        "\n",
        "    # Convert color space and resize\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    H, W, _ = image.shape\n",
        "    image = cv2.resize(image, (IMG_SIZE, IMG_SIZE))\n",
        "    image = image / 255.0  # Normalize to [0, 1]\n",
        "\n",
        "    # Normalize bounding box coordinates\n",
        "    bbox = np.array([\n",
        "        x / W,           # x_min\n",
        "        y / H,           # y_min\n",
        "        (x + w) / W,     # x_max\n",
        "        (y + h) / H      # y_max\n",
        "    ], dtype=np.float32)\n",
        "\n",
        "    return image.astype(np.float32), bbox"
      ],
      "metadata": {
        "id": "qyCNm98T1RRs"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DATA AUGMENTATION"
      ],
      "metadata": {
        "id": "0mdpGjE2TW2K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply augmentation only to training data\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "    tf.keras.layers.RandomFlip(\"horizontal\"),\n",
        "    tf.keras.layers.RandomBrightness(0.2),\n",
        "    tf.keras.layers.RandomContrast(0.2),\n",
        "    tf.keras.layers.RandomZoom(0.1)\n",
        "])"
      ],
      "metadata": {
        "id": "1LbWgPkjS5wa"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CREATE DATASETS"
      ],
      "metadata": {
        "id": "u6beSAgZTbId"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_generator():\n",
        "    for s in train_samples:\n",
        "        img, bbox = load_and_preprocess(s, img_base_dir)\n",
        "        img = data_augmentation(img)\n",
        "        yield img, bbox\n",
        "\n",
        "def val_generator():\n",
        "    for s in val_samples:\n",
        "        img, bbox = load_and_preprocess(s, img_base_dir)\n",
        "        yield img, bbox"
      ],
      "metadata": {
        "id": "mp6LiO6eTaoC"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create TensorFlow datasets\n",
        "train_ds = tf.data.Dataset.from_generator(\n",
        "    train_generator,\n",
        "    output_signature=(\n",
        "        tf.TensorSpec(shape=(IMG_SIZE, IMG_SIZE, 3), dtype=tf.float32),\n",
        "        tf.TensorSpec(shape=(4,), dtype=tf.float32)\n",
        "    )\n",
        ").shuffle(512).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "val_ds = tf.data.Dataset.from_generator(\n",
        "    val_generator,\n",
        "    output_signature=(\n",
        "        tf.TensorSpec(shape=(IMG_SIZE, IMG_SIZE, 3), dtype=tf.float32),\n",
        "        tf.TensorSpec(shape=(4,), dtype=tf.float32)\n",
        "    )\n",
        ").batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "taQIBkrVS-Fj"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BUILD MODELS"
      ],
      "metadata": {
        "id": "GM4te7JnTkZI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "MODEL 1 : CNN FROM SCRATCH"
      ],
      "metadata": {
        "id": "zumdY3F5Tyak"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_scratch_model():\n",
        "    \"\"\"Simple CNN with 3 convolutional layers\"\"\"\n",
        "    return tf.keras.Sequential([\n",
        "        tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=(224,224,3)),\n",
        "        tf.keras.layers.MaxPooling2D(),\n",
        "        tf.keras.layers.Conv2D(64, 3, activation='relu'),\n",
        "        tf.keras.layers.MaxPooling2D(),\n",
        "        tf.keras.layers.Conv2D(128, 3, activation='relu'),\n",
        "        tf.keras.layers.MaxPooling2D(),\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.Dense(512, activation='relu'),\n",
        "        tf.keras.layers.Dense(4, activation='sigmoid')  # Output: 4 bbox coordinates\n",
        "    ])"
      ],
      "metadata": {
        "id": "sc6LMexzTh3W"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scratch_model = build_scratch_model()\n",
        "scratch_model.compile(optimizer=tf.keras.optimizers.Adam(1e-4), loss=\"mse\")\n",
        "\n",
        "scratch_history = scratch_model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=5\n",
        ")"
      ],
      "metadata": {
        "id": "xCN_qev_Tu19"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MODEL 2: VGG16"
      ],
      "metadata": {
        "id": "lAbFSXjUT8Qg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_vgg16_model():\n",
        "    \"\"\"VGG16 pre-trained on ImageNet, adapted for bounding box regression\"\"\"\n",
        "    base = VGG16(weights=\"imagenet\", include_top=False, input_shape=(224,224,3))\n",
        "    base.trainable = False  # Freeze pre-trained layers\n",
        "\n",
        "    x = tf.keras.layers.Flatten()(base.output)\n",
        "    x = tf.keras.layers.Dense(512, activation=\"relu\")(x)\n",
        "    output = tf.keras.layers.Dense(4, activation=\"sigmoid\")(x)\n",
        "    return tf.keras.Model(base.input, output)"
      ],
      "metadata": {
        "id": "FvjX6H25T38K"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vgg_model = build_vgg16_model()\n",
        "vgg_model.compile(optimizer=tf.keras.optimizers.Adam(1e-4), loss=\"mse\")\n",
        "\n",
        "vgg_history = vgg_model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=5\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vf84g6F7poJS",
        "outputId": "560e71b5-0ca1-4b86-abc5-48e6ba4698ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3121s\u001b[0m 12s/step - loss: 0.0769 - val_loss: 0.0640\n",
            "Epoch 2/5\n",
            "\u001b[1m 31/250\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m36:49\u001b[0m 10s/step - loss: 0.0522"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MODEL 3: RESNET50"
      ],
      "metadata": {
        "id": "UpXpCvaBpvK9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_resnet_model():\n",
        "    \"\"\"ResNet50 pre-trained on ImageNet, adapted for bounding box regression\"\"\"\n",
        "    base = ResNet50(weights=\"imagenet\", include_top=False, input_shape=(224,224,3))\n",
        "    base.trainable = False  # Freeze pre-trained layers\n",
        "\n",
        "    x = tf.keras.layers.GlobalAveragePooling2D()(base.output)\n",
        "    x = tf.keras.layers.Dense(512, activation=\"relu\")(x)\n",
        "    output = tf.keras.layers.Dense(4, activation=\"sigmoid\")(x)\n",
        "    return tf.keras.Model(base.input, output)"
      ],
      "metadata": {
        "id": "SZQQIGrSpsiJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resnet_model = build_resnet_model()\n",
        "resnet_model.compile(optimizer=tf.keras.optimizers.Adam(1e-4), loss=\"mse\")\n",
        "\n",
        "resnet_history = resnet_model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=5\n",
        ")"
      ],
      "metadata": {
        "id": "eR9UOiGgp8O2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### EVALUATE AND COMPARE MODELS"
      ],
      "metadata": {
        "id": "PxbDR6G_qQhZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scratch_loss = scratch_model.evaluate(val_ds)\n",
        "vgg_loss = vgg_model.evaluate(val_ds)\n",
        "resnet_loss = resnet_model.evaluate(val_ds)\n",
        "\n",
        "print(f\"\\nCNN from Scratch - Loss: {scratch_loss:.6f}\")\n",
        "print(f\"VGG16 - Loss: {vgg_loss:.6f}\")\n",
        "print(f\"ResNet50 - Loss: {resnet_loss:.6f}\")"
      ],
      "metadata": {
        "id": "XzSsow4BqZRk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot comparison\n",
        "plt.figure(figsize=(10, 6))\n",
        "models = ['CNN from Scratch', 'VGG16', 'ResNet50']\n",
        "losses = [scratch_loss, vgg_loss, resnet_loss]\n",
        "plt.bar(models, losses, color=['#1f77b4', '#ff7f0e', '#2ca02c'])\n",
        "plt.ylabel('Validation Loss (MSE)')\n",
        "plt.title('Model Comparison - Lower is Better')\n",
        "plt.grid(axis='y', alpha=0.3)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9uEHykW8qjIS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### VISUALIZE PREDICTIONS FROM ALL MODELS"
      ],
      "metadata": {
        "id": "afMSGJpJqkbc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_and_draw_box(model, sample):\n",
        "    \"\"\"Predict face location and draw bounding box on image\"\"\"\n",
        "    img, true_bbox = load_and_preprocess(sample, img_base_dir)\n",
        "\n",
        "    # Predict bounding box\n",
        "    pred_bbox = model.predict(np.expand_dims(img, 0), verbose=0)[0]\n",
        "\n",
        "    # Convert normalized coordinates to pixels\n",
        "    h, w = IMG_SIZE, IMG_SIZE\n",
        "\n",
        "    # Ground truth box (GREEN)\n",
        "    x1_true = int(true_bbox[0] * w)\n",
        "    y1_true = int(true_bbox[1] * h)\n",
        "    x2_true = int(true_bbox[2] * w)\n",
        "    y2_true = int(true_bbox[3] * h)\n",
        "\n",
        "    # Predicted box (RED)\n",
        "    x1_pred = int(pred_bbox[0] * w)\n",
        "    y1_pred = int(pred_bbox[1] * h)\n",
        "    x2_pred = int(pred_bbox[2] * w)\n",
        "    y2_pred = int(pred_bbox[3] * h)\n",
        "\n",
        "    # Draw both boxes on image\n",
        "    img_with_box = (img * 255).astype(np.uint8).copy()\n",
        "    cv2.rectangle(img_with_box, (x1_true, y1_true), (x2_true, y2_true), (0, 255, 0), 2)  # Green = Ground Truth\n",
        "    cv2.rectangle(img_with_box, (x1_pred, y1_pred), (x2_pred, y2_pred), (255, 0, 0), 2)  # Red = Prediction\n",
        "\n",
        "    return img_with_box"
      ],
      "metadata": {
        "id": "qkCfm4gOqoIr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show 5 sample images with predictions from all 3 models\n",
        "num_samples = 5\n",
        "fig, axes = plt.subplots(num_samples, 3, figsize=(15, num_samples * 5))\n",
        "fig.suptitle('Face Detection Comparison - Green=Truth, Red=Prediction', fontsize=16)\n"
      ],
      "metadata": {
        "id": "H3OWy4EhqxOA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, sample in enumerate(val_samples[:num_samples]):\n",
        "    # CNN from Scratch\n",
        "    result_scratch = predict_and_draw_box(scratch_model, sample)\n",
        "    axes[i, 0].imshow(result_scratch)\n",
        "    axes[i, 0].set_title(f'Sample {i+1} - CNN from Scratch')\n",
        "    axes[i, 0].axis('off')\n",
        "\n",
        "    # VGG16\n",
        "    result_vgg = predict_and_draw_box(vgg_model, sample)\n",
        "    axes[i, 1].imshow(result_vgg)\n",
        "    axes[i, 1].set_title(f'Sample {i+1} - VGG16')\n",
        "    axes[i, 1].axis('off')\n",
        "\n",
        "    # ResNet50\n",
        "    result_resnet = predict_and_draw_box(resnet_model, sample)\n",
        "    axes[i, 2].imshow(result_resnet)\n",
        "    axes[i, 2].set_title(f'Sample {i+1} - ResNet50')\n",
        "    axes[i, 2].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "maOsaHU5rN78"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w3Z6SSp9rRgB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}